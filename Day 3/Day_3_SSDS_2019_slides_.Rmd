---
title: "3rd Day Workshop - Dimensionality Reduction"
date: "11 September 2019"
output:
  # ioslides_presentation:
  # beamer_presentation:
  #   highlight: haddock
  # html_document: default
  # '# ioslides_presentation': default
  #   fig_height: 4
  #   highlight: haddock
  #   smaller: yes
  #   transition: slower
  #   widescreen: yes
---

<style>
h2 { 
 font-size:1.3em; 
}
</style>

```{r include=FALSE}
# setup chunk

Sys.setlocale("LC_ALL","English")

library(dplyr)
library(tidyverse)
library(lubridate)
library(foreach)
library(e1071)
library(ggsci)
library(ggpubr)
library(tidyquant)

library(ca)
library(psych)
library(MVN)
library(psy)
library(ICtest)

library(ggcorrplot)
library(corrplot)

library(broom)

library(tibble)
library(GPArotation)

library(Hmisc)

knitr::opts_chunk$set(results = 'hold')
```

## Dimensionality Reduction Techiques 

Dimensionality reduction (linear): 

* **Principal Component Analysis (PCA)** - decompose data to linear vectors looking to explain **total variance** of the data (behavioral variability)
* **Factor Analysis (FA)** - find underlying latent factor that their linear combination acount for **common variance** of the data (model based)

## FA and PCA - practical implication

* behavioral modeling (behavioral variance)
* independent components for clustering (e.g. k-means centering)
* database collection imbalance problem (online vs offline)
* analysis of business process

## Dimensionality Reduction I - Principal Component Analysis (PCA)

PCA (Karhunen-Loeve Transform)  - project the data onto a lower dimensional linear space such that the **variance of the projected data is maximized** (orthogonal vectors)

cost: linear projection that minimizes the average projection cost (mean squared distance between the data points and their projections)

Commonly calculated by SVD (Singular Value Decomposition)

```{r, eval = T, tidy = T, warning = F}
# Import dataset and show the header
data_in <- read.csv('data_in_F_r_out.csv')
data_in$X <- NULL

as_tibble(data_in)
```


## Analyse the Dataset

World Happiness Country Report 2018

```{r, eval = T, tidy = T, warning = F}
# Describe the dataset by psych package 
psych::describe(data_in)
```

## Target variable distribution with median

```{r, eval = T, tidy = T, warning = F}
# Show the distribution of the happiness score

data_in %>% ggplot(aes(x = Happiness_score)) + geom_density(fill = 'dodgerblue') + geom_vline(xintercept = median(data_in$Happiness_score)) + theme_bw()

```

## Locate those happy campers

```{r, eval = T, tidy = T, warning = F}
# Top 5 most happy

data_in %>% top_n(n = 5, Happiness_score)


```

## Locate those negative countries

```{r, eval = T, tidy = T, warning = F}
# Top 5 most unhapppy

data_in %>% top_n(n = -5, Happiness_score)

```

## Locate Croatia

```{r, eval = T, tidy = T, warning = F}
# sunny Croatia

data_in %>% filter(Country == 'Croatia')

```


## PCA analysis preprocesing - scaling data 

```{r, eval = T, tidy = T, warning = F}
Happiness_score_save <- data_in$Happiness_score
data_in$Happiness_score <- NULL

Country_save <- data_in$Country
data_in$Country <- NULL

data_in_F_r_sc <- scale(data_in, center = T, scale = T)

# check some stats
data_in_F_r_sc_df <- as.data.frame(data_in_F_r_sc)

mean(data_in_F_r_sc_df$Life_Ladder)
sd(data_in_F_r_sc_df$Life_Ladder)


```

## PCA side - Data correlation matrix

Data structure comes from features not beeing independent

```{r, eval = T, tidy = T, warning = F}
cor_mx <- rcorr(as.matrix(data_in_F_r_sc))
head(cor_mx$r)

```

## Visualize the correlation matrix

```{r, eval = T, tidy = T, warning = F}
corrplot(cor_mx$r, method = "pie")

```

## Visualize the correlation matrix - nice number view

```{r, eval = T, tidy = T, warning = F}
corrplot(cor_mx$r, method = "number", number.cex = 0.5)

```

## Visualize the correlation matrix - clustering visualisation

```{r, eval = T, tidy = T, warning = F}
p_vals_corr <- cor_mx$P
corrplot(cor_mx$r, order  = "hclust", p.mat = p_vals_corr, sig.level = 0.01)

```


## Visualize the correlation matrix - LM() point for reminder?

```{r, eval = T, tidy = T, warning = F}
lm_mod_ex <- lm(Life_expectancy ~ Log_GDPpC, data = as.data.frame(data_in_F_r_sc))
summary(lm_mod_ex)

```


## PCA analysis - eigenvalues
```{r, eval = T, tidy = T, warning = F}
pca_res <- prcomp(data_in_F_r_sc, center = F, scale. = F)

# get eigenvalue - squared st.dev
eig_val <- pca_res$sdev^2
as.data.frame(eig_val) %>% ggplot(aes(x = as.factor(index(eig_val)), y = eig_val)) + geom_col(fill = 'steelblue') + theme_bw() + geom_hline(aes(yintercept = 1))

```

## PCA analysis - explanied and cumulative variance
```{r, eval = T, tidy = T, warning = F}
# get proportions of explained variance and cumulative variance
eig_prop <- eig_val/sum(eig_val)
eig_prop_cum <- cumsum(eig_prop)

as.data.frame(eig_prop_cum) %>% ggplot(aes(x = as.factor(index(eig_prop_cum)), y = eig_prop_cum)) + geom_col(fill = 'steelblue')+theme_bw() + geom_hline(aes(yintercept = 0.9))

```

## PCA analysis - exciting code momentarily
```{r, eval = T, tidy = T, warning = F}
# get eigenvectors
eig_vec <- pca_res$rotation
# get coordinates of variables (colums)
var_coord_fun <- function(loadings, comp_sdev){
  ret_val = loadings*comp_sdev
  ret_val}
variable_coord <- t(apply(eig_vec, 1, var_coord_fun, pca_res$sdev)) 
# get quality of representation - squared coordinates
var_cos2 <- variable_coord^2
# get contribution for each variable
# 1. get sum on colums
var_cos2_sum <- apply(var_cos2, 2, sum)
# 2. get relevat proportion for each variable = cos2/sum_on_colums
contrib_fun <- function(var_cos2, var_cos2_sum){var_cos2*100/var_cos2_sum}

var_contrib <- t(apply(var_cos2,1, contrib_fun, var_cos2_sum))
# check calculation - needs to be 100 percent
var_contrib_ch <- apply(var_contrib, 2, sum)
var_contrib_ch

```

## PCA analysis - lets plot top 2 profiles
```{r, eval = T, tidy = T, warning = F}
ind_PCA <- pca_res$x
ind_PCA <- as.data.frame(ind_PCA)
ind_PCA_all <- ind_PCA %>% bind_cols(data_in)
ind_PCA_all$Happiness_score <- Happiness_score_save
ind_PCA_all %>% ggplot(aes(x = PC1, y = PC2, color = Happiness_score, label = Country_save)) + geom_point() + geom_text(vjust = 0, nudge_y = 0.5) + theme_bw()

```

## PCA analysis - get those correlations
```{r, eval = T, tidy = T, warning = F}
corref <- cor(ind_PCA_all[,13:ncol(ind_PCA_all)], ind_PCA_all[,1:12], method = 'pearson')
corr_plot_dim <- ggcorrplot(t(corref),
                            outline.col = "black",
                            ggtheme = ggplot2::theme_gray,
                            colors = c("#E46726", "white", "#6D9EC1"))
corr_plot_dim
```

## PCA analysis - correlations (corr)
```{r, eval = T, tidy = T, warning = F}
head(corref)
```

## PCA analysis - correlations (variable coordination)
```{r, eval = T, tidy = T, warning = F}
head(variable_coord)
```



## PCA analysis - LM model prep + contribution plot
```{r, eval = T, tidy = T, warning = F}
ind_PCA_lm <- ind_PCA;
ind_PCA_lm$Happiness_score <- Happiness_score_save;
lm_mod <- lm(Happiness_score ~ ., data = ind_PCA_lm);

corrplot(var_contrib, is.corr=FALSE ,tl.cex = 0.7) 
```

## PCA analysis - LM model insights
```{r, eval = T, tidy = T, warning = F}
cor(ind_PCA_lm$Happiness_score, ind_PCA_lm$PC1)
summary(lm_mod)
```

## PCA analysis - LM model insights
```{r, eval = T, tidy = T, warning = F}
lm_mod2 <- lm(Happiness_score ~ PC1 + PC2 + PC3 + PC4 + PC5 + PC6 + PC7, data = ind_PCA_lm);
summary(lm_mod2)
```
## Dimensionality Reduction II -  Factor Analysis (FA)

FA assumes a statistical model that describes covariation in observed variables via linear combinations of latent variables.

note: we will be doing Exploratory Factor Analysis - (as dimensionality reduction techique) 

Tests first:

1. Bartlett's test of Sphericity that a correlation matrix is an identity matrix (indicate that your variables are unrelated)
2. Kaiser-Meyer-Olkin (KMO) Test - measure of the proportion of variance among variables that might be common variance. The higher the value, the more suited your data is to factor analysis.

* 0.00 to 0.49 unacceptable
* 0.50 to 0.59 miserable
* 0.60 to 0.69 mediocre
* 0.70 to 0.79 middling
* 0.80 to 0.89 meritorious
* 0.90 to 1.00 marvelous

## Factor Analysis - Bartlett test

```{r, eval = T, tidy = T, warning = F}
# correlation matrix
corr_mx <- cor(data_in_F_r_sc_df)

# Bartlett
bartlett_res <- cortest.bartlett(corr_mx, n = 200, diag=TRUE)
bartlett_res

```

## Factor Analysis - Kaiser-Meyer-Olkin (KMO) test
```{r, eval = T, tidy = T, warning = F}

# Kaiser-Meyer-Olkin (KMO) Test
KMO_res <- KMO(corr_mx)
KMO_res

```

## Factor Analysis - Normality test

Don't use maximum likelihood if not normal, but use principal axis factoring (PAF) by Brown,2015

```{r, eval = T, tidy = T, warning = F}
# Mardia’s multivariate normality test + Shapiro test
mvn_test <- mvn(data_in_F_r_sc_df, mvnTest = c("mardia"), multivariatePlot = "qqplot")
mvn_test$multivariateNormality
mvn_test$univariateNormality

```

## Factor Analysis - But how much factors?

* Test with scree plot vs random data
* Let's confirm PCA also

```{r, eval = T, tidy = T, warning = F}
#  parallel analys with random data - sugested 3 factors
fa.parallel(corr_mx, n.obs = dim(data_in_F_r_sc_df)[1], fm="pa")

```

## FA with only 1 factor 

```{r, eval = T, tidy = T, warning = F}
fit_f1 <- fa(corr_mx, nfactors = 1, rotate = "varimax", fm="pa")
print(fit_f1)

```

## FA with 3 factors

```{r, eval = T, tidy = T, warning = F}
fit_f3 <- fa(corr_mx, nfactors = 3, rotate = "varimax", fm="pa")
print(fit_f3)
```

## FA with 3 factors - loadings

```{r, eval = T, tidy = T, warning = F}
print(fit_f3$loadings,cutoff = 0.3)
```

## FA with 3 factors - fa.diagram

```{r, eval = T, tidy = T, warning = F}
fa.diagram(fit_f3, simple= F)
```

## FA with 3 factors - get residual

```{r, eval = T, tidy = T, warning = F}
# get residual
head(fit_f3$residual)
```

## FA with 3 factors - get communality

Communality - percentage of variance that can be explained by the retained factors for each variable

```{r, eval = T, tidy = T, warning = F}
fit_f3$communality
```

## FA with 3 factors - check some fits

* How well does the factor model reproduce the correlation matrix?

* How well are the off diagonal elements reproduced?

```{r, eval = T, tidy = T, warning = F}
# corr matrix reproduction
fit_f3$fit
# off diagonal elements reproduction
fit_f3$fit.off
```

## FA with 3 factors - check other rotations (oblimin)
```{r, eval = T, tidy = T, warning = F}
fit_f3b <- fa(corr_mx, nfactors = 3, rotate = "oblimin", fm="pa")
print(fit_f3b)
```

## FA with 3 factors - check other rotations (quartimax)
```{r, eval = T, tidy = T, warning = F}
fit_f3c <- fa(corr_mx, nfactors = 3, rotate = "quartimax", fm="pa")
print(fit_f3c)
```

## FA with 3 factors - check loadings (varimax)

* Varimax Method. An orthogonal rotation method that minimizes the number of variables that have high loadings on each factor

```{r, eval = T, tidy = T, warning = F}
print(fit_f3$loadings,cutoff = 0.3)
```


## FA with 3 factors - check loadings (oblimin)

* Direct Oblimin Method. A method for oblique (nonorthogonal) rotation.
* but not to much correlation, as then both of them can be approximatzed by just one factor

```{r, eval = T, tidy = T, warning = F}
print(fit_f3b$loadings,cutoff = 0.3)
```


## FA with 3 factors - check loadings (quartimax)

* Quartimax Method. A rotation method that minimizes the number of factors needed to explain each variable. This method simplifies the interpretation of the observed variables. 

```{r, eval = T, tidy = T, warning = F}
print(fit_f3c$loadings,cutoff = 0.3)
```

## FA with 3 factors - regression score and weights for factors  (varimax)
```{r, eval = T, tidy = T, warning = F}
scores_FA <- factor.scores(data_in_F_r_sc_df, fit_f3$loadings, method = c("Thurstone"))
scores_val <- scores_FA$scores
```

## FA with 3 factors - regression score and weights for factors (oblimin)
```{r, eval = T, tidy = T, warning = F}
scores_FA2 <- factor.scores(data_in_F_r_sc_df, fit_f3b$loadings, method = c("Thurstone"))
scores_val2 <- scores_FA2$scores
```

## FA with 3 factors - check r.scores (varimax)

* r.scores - The correlations of the factor score estimates
* Should be nonexistent correlation if varimax is used across factors

```{r, eval = T, tidy = T, warning = F}
r.scores_val <- scores_FA$r.scores
r.scores_val
```

## FA with 3 factors - check r.scores (oblimin)

* Tabachnick and Fiddell (2007, p. 646) - "Look at  the  factor  correlation  matrix  for  correlations  around  .32  and  above.  If  correlations  exceed  .32,  then there is 10% (or more) overlap in variance among factors (0.32 sqrd), enough variance to warrant oblique rotation unless there are compelling reasons for orthogonal rotation."

* Let's use only varimax

```{r, eval = T, tidy = T, warning = F}
r.scores_val2 <- scores_FA2$r.scores
r.scores_val2
```

## FA with 3 factors - check some  hierarchical decisions - iclust model for validation

* iclust model - hirearchical cluster of feature variables correlation
* alternative to factor analysis
* Clusters are combined if coefficients alpha and beta will increase in the new cluster.
* Alpha - the mean split half correlation 
* Beta - the worst split half correlation 
* they are estimates of the reliability and general factor saturation of the test. 

## FA with 3 factors - check some  hierarchical decisions - iclust model visualization
```{r, eval = T, tidy = T, warning = F}
fit_ic <- iclust(corr_mx)
```

## FA with 3 factors - check some  hierarchical decisions - iclust model summary
```{r, eval = T, tidy = T, warning = F}
summary(fit_ic) 
```

## FA with 3 factors - check with Very Simple Structure

* applies a goodness of fit test to determine the optimal number of factors to extract
* all except the biggest X loadings per item are set to zero where X is the level of complexity of the item

```{r, eval = T, tidy = T, warning = F}
# check with vss 
vss <- vss(corr_mx,title="Very Simple Structure", rotate = "varimax", n.obs=dim(data_in_F_r_sc_df)[1], fm = "pa", SMC=FALSE)
vss  
```

## FA with 3 factors - Very Simple Structure summary
```{r, eval = T, tidy = T, warning = F}
vss 
```

## FA with 3 factors - get lm for FA scores

* What factors explain dataset? (unsupervised)
* What factors contribute to the target variable? (LM) (supervised)

```{r, eval = T, tidy = T, warning = F}
scores_val_lm <- as.data.frame(scores_val);
scores_val_lm$Happiness_score <- Happiness_score_save;

lm_mod_FA <- lm(Happiness_score ~ ., data = scores_val_lm);
```

## FA with 3 factors - get lm summary

Note: lower R square but generally more coherent understanding of the underlying driver of the behavior (closer to simple structure)

```{r, eval = T, tidy = T, warning = F}
summary(lm_mod_FA)
```

## FA with 3 factors - get lm for FA scores

```{r, eval = T, tidy = T, warning = F}
scores_val_lm %>% ggplot(aes(x = PA1, y = PA2, color = Happiness_score, label = Country_save)) + geom_point() + geom_text(vjust = 0, nudge_y = 0.5) + theme_bw()

```


## FA with 3 factors - get correlation FA vs PCA
```{r, eval = T, tidy = T, warning = F}
corref_FA_PCA <- cor(scores_val, ind_PCA, method = 'pearson')
corr_plot_FA_PCA <- ggcorrplot(t(corref_FA_PCA),
                            outline.col = "black",
                            ggtheme = ggplot2::theme_gray,
                            colors = c("#E46726", "white", "#6D9EC1"), lab = T)
corr_plot_FA_PCA
 
```

## Factor Analysis (FA) and Principal Component Analysis (PCA) - summary

* both unsupervised
* analyses  different target (common variance vs total variance)
* PCA - decomposition, FA - model based
* PCA - no component assumption, FA - assumption of underlying drivers (but can be tested)
* no clear cut decision rule for number of components / factors (well maybe...)
* eigenvaule = 1, total variance = 0.9, compare to random data ... all valid logic
* FA rotation approach - no clear winner irl (varimax can be to strict sometimes but it is the most common)

